{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02f9165",
   "metadata": {},
   "source": [
    "# LLM Inference Pipeline\n",
    "\n",
    "This notebook implements the LLM inference pipeline described in `pipeline_spec.md`.\n",
    "\n",
    "Sections:\n",
    "- Setup & installs\n",
    "- Parse and validate spec\n",
    "- Tokenizer & Model loader\n",
    "- Prompt construction\n",
    "- Dataset loading & sampling\n",
    "- Batched inference & streaming generator\n",
    "- Extraction, metrics, and plotting\n",
    "- Demo runs and simple tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048ec781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_pipeline:Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup: Install dependencies and import packages\n",
    "\n",
    "# If running in binder or a fresh environment you may need to install packages.\n",
    "# Use `%pip install` to ensure installs are available in the notebook kernel.\n",
    "\n",
    "# Uncomment and run if you need to install packages\n",
    "# %pip install -q transformers accelerate torch pandas matplotlib seaborn datasets evaluate tqdm regex sentencepiece\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Try importing transformers and torch and give friendly guidance if missing\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForCausalLM,\n",
    "        pipeline,\n",
    "        logging as hf_logging,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Transformers or Torch not available in the kernel. Please run the install cell: `%pip install transformers torch`\"\n",
    "    ) from e\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"llm_pipeline\")\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Device detection\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    ")\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Reproducible seeds\n",
    "DEFAULT_SEED = 42\n",
    "random.seed(DEFAULT_SEED)\n",
    "torch.manual_seed(DEFAULT_SEED)\n",
    "\n",
    "# Default paths / constants\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\"\n",
    "PIPELINE_SPEC = PROJECT_ROOT / \"pipeline_spec.md\"\n",
    "DEFAULT_MODEL = (\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\"  # per spec; we will fallback if unavailable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef7648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading spec from: /Users/paultalma/Documents/UCLA/Work/Courses/2025-2026/cs_263/project_repo/pipeline_spec.md\n",
      "# pipeline spec\n",
      "\n",
      "- load dataset\n",
      "    - place dataset into memory\n",
      "- sample k from dataset\n",
      "    - selects k data points from the dataset\n",
      "    - k should be a variable parameter\n",
      "    - should be able to select whether the sampling is random or from the first k\n",
      "    - random sampling should be seeded\n",
      "- make prompts\n",
      "    - prompts should be structured as follows:\n",
      "        - system: \"You are a news trustworthiness classifier.\\n\"\n",
      "          \"Your task is to classify news articles as either trustworthy or untrustworthy.\\n\"\n",
      "          \"Your answer should consist of exactly one token: 0 if the article is untrustworthy and 1 if it is trustworthy.\\n\\n\"\n",
      "        - user: \"ARTICLE: {article}\\n\\n\"\n",
      "          \"CLASSIFICATION (0 or 1):\"\n",
      "\n",
      "- load model\n",
      "    - this should be modular, to make it easy to switch between models\n",
      "    - default model should be \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
      "- load tokenizer\n",
      "    - the tokenizer should have a variable context length\n",
      "- run model\n",
      "    - this runs the model on the k loaded samples, collecting model outputs\n",
      "- extract classifications\n",
      "    - this should extract a classification from the model output (a simple regex or _ is in _ should suffice)\n",
      "- compute metrics\n",
      "    - simple classification metrics (accuracy)\n",
      "- display graphs\n",
      "    - this should take in the metrics corresponding to different models and display them on a common chart\n",
      "\n",
      "\n",
      "Parsed pipeline spec: PipelineSpec(default_model='Qwen/Qwen2.5-0.5B-Instruct', tokenizer_context_length=2048, classification_system_prompt='You are a news trustworthiness classifier.\\nYour task is to classify news articles as either trustworthy or untrustworthy.\\nYour answer should consist of exactly one token: 0 if the article is untrustworthy and 1 if it is trustworthy.\\n\\n', user_template='ARTICLE: {article}\\n\\nCLASSIFICATION (0 or 1):')\n"
     ]
    }
   ],
   "source": [
    "# Load and parse pipeline_spec.md\n",
    "\n",
    "print(f\"Reading spec from: {PIPELINE_SPEC}\")\n",
    "try:\n",
    "    with open(PIPELINE_SPEC, \"r\") as f:\n",
    "        spec_text = f.read()\n",
    "        print(spec_text)\n",
    "except Exception as e:\n",
    "    logger.warning(\n",
    "        \"Could not read pipeline_spec.md; proceeding with default inferred spec.\"\n",
    "    )\n",
    "    spec_text = \"\"\n",
    "\n",
    "\n",
    "# Quick parser: extract main required fields\n",
    "@dataclass\n",
    "class PipelineSpec:\n",
    "    default_model: str = DEFAULT_MODEL\n",
    "    tokenizer_context_length: int = 2048\n",
    "    classification_system_prompt: str = (\n",
    "        \"You are a news trustworthiness classifier.\\n\"\n",
    "        \"Your task is to classify news articles as either trustworthy or untrustworthy.\\n\"\n",
    "        \"Your answer should consist of exactly one token: 0 if the article is untrustworthy and 1 if it is trustworthy.\\n\\n\"\n",
    "    )\n",
    "    user_template: str = \"ARTICLE: {article}\\n\\nCLASSIFICATION (0 or 1):\"\n",
    "    # more fields could be added\n",
    "\n",
    "\n",
    "pipeline_spec = PipelineSpec()\n",
    "print(\"Parsed pipeline spec:\", pipeline_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b17c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility helpers: logger, retry/backoff, cache\n",
    "\n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "\n",
    "def retry_backoff(retries: int = 3, initial_delay: float = 0.5, factor: float = 2.0):\n",
    "    def decorator(fn):\n",
    "        @wraps(fn)\n",
    "        def inner(*args, **kwargs):\n",
    "            delay = initial_delay\n",
    "            for i in range(retries):\n",
    "                try:\n",
    "                    return fn(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Call failed (attempt {i+1}/{retries}): {e}\")\n",
    "                    if i == retries - 1:\n",
    "                        raise\n",
    "                    time.sleep(delay)\n",
    "                    delay *= factor\n",
    "\n",
    "        return inner\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class SimpleCache:\n",
    "    def __init__(self):\n",
    "        self._cache = {}\n",
    "\n",
    "    def get(self, key):\n",
    "        return self._cache.get(key)\n",
    "\n",
    "    def set(self, key, value):\n",
    "        self._cache[key] = value\n",
    "\n",
    "\n",
    "cache = SimpleCache()\n",
    "\n",
    "\n",
    "# Small structured logger wrapper\n",
    "def log_info(msg: str):\n",
    "    logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53cb54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_pipeline:Loaded tokenizer for: Qwen/Qwen2.5-0.5B-Instruct with max_length=2048\n",
      "INFO:llm_pipeline:Tokenizer round-trip test passed.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer setup and verification\n",
    "\n",
    "\n",
    "def load_tokenizer(model_name: str, max_length: Optional[int] = None):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            # Ensure pad token exists for batching\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "        if max_length is not None:\n",
    "            tokenizer.model_max_length = max_length\n",
    "        logger.info(\n",
    "            f\"Loaded tokenizer for: {model_name} with max_length={tokenizer.model_max_length}\"\n",
    "        )\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to load tokenizer for {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Round-trip test helper\n",
    "\n",
    "\n",
    "def tokenizer_round_trip_test(tokenizer):\n",
    "    txt = \"This is a short test sentence.\"\n",
    "    encoded = tokenizer(txt)\n",
    "    decoded = tokenizer.decode(encoded.input_ids)\n",
    "    assert isinstance(decoded, str)\n",
    "    assert txt == decoded\n",
    "    logger.info(\"Tokenizer round-trip test passed.\")\n",
    "\n",
    "\n",
    "# Example default tokenizer load (with safe fallback)\n",
    "try:\n",
    "    default_tokenizer = load_tokenizer(\n",
    "        pipeline_spec.default_model, max_length=pipeline_spec.tokenizer_context_length\n",
    "    )\n",
    "except Exception:\n",
    "    logger.info(\"Falling back to 'gpt2' tokenizer for demo purposes.\")\n",
    "    default_tokenizer = load_tokenizer(\"gpt2\", max_length=1024)\n",
    "\n",
    "# run simple test\n",
    "tokenizer_round_trip_test(default_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b38a075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_pipeline:Loaded model gpt2 to mps\n",
      "INFO:llm_pipeline:Smoke model loaded for demo runs.\n"
     ]
    }
   ],
   "source": [
    "# Model loader: device placement, caching, and smoke tests\n",
    "\n",
    "\n",
    "@retry_backoff(retries=2)\n",
    "def load_model(\n",
    "    model_name: str, device: str = DEVICE, dtype: Optional[torch.dtype] = None\n",
    "):\n",
    "    \"\"\"Load a causal LM model with graceful fallback.\n",
    "\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    # Check cache\n",
    "    cache_key = f\"model::{model_name}::device::{device}\"\n",
    "    cached = cache.get(cache_key)\n",
    "    if cached is not None:\n",
    "        logger.info(f\"Using cached model for {model_name}\")\n",
    "        return cached\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        # Add pad token if missing\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        cache.set(cache_key, (model, tokenizer))\n",
    "        logger.info(f\"Loaded model {model_name} to {device}\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to load model {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Safe loader that falls back to a small model\n",
    "\n",
    "\n",
    "def load_model_safe(\n",
    "    preferred: str = pipeline_spec.default_model, fallback: str = \"gpt2\"\n",
    ") -> Tuple[Any, Any]:\n",
    "    try:\n",
    "        return load_model(preferred)\n",
    "    except Exception:\n",
    "        logger.info(f\"Falling back to {fallback}\")\n",
    "        return load_model(fallback)\n",
    "\n",
    "\n",
    "# Try a smoke load of a small model for the demo\n",
    "smoke_model_name = \"gpt2\"\n",
    "smoke_model, smoke_tokenizer = load_model_safe(smoke_model_name)\n",
    "logger.info(\"Smoke model loaded for demo runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1aadb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a news trustworthiness classifier.\n",
      "Your task is to classify news articles as either trustworthy or untrustworthy.\n",
      "Your answer should consist of exactly one token: 0 if the article is untrustworthy and 1 if it is trustworthy.\n",
      "\n",
      "\n",
      "ARTICLE: This is a short example article.\n",
      "\n",
      "CLASSIFICATION (0 or 1):\n"
     ]
    }
   ],
   "source": [
    "# Prompt templates and preprocessing\n",
    "\n",
    "SYSTEM_PROMPT = pipeline_spec.classification_system_prompt\n",
    "USER_TEMPLATE = pipeline_spec.user_template\n",
    "\n",
    "\n",
    "def make_prompt(\n",
    "    article: str, system: str = SYSTEM_PROMPT, user_template: str = USER_TEMPLATE\n",
    ") -> str:\n",
    "    \"\"\"Create a single-string prompt combining system and user prompts.\n",
    "    Many HF models accept a single string; some support role-based chat models instead.\n",
    "    \"\"\"\n",
    "    user_text = user_template.format(article=article)\n",
    "    prompt = system + \"\\n\" + user_text\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Example\n",
    "print(make_prompt(\"This is a short example article.\")[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002aa8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22216</th>\n",
       "      <td>21st Century Wire says Ben Stein, reputable pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27917</th>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25007</th>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rosse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>On Monday, Donald Trump once again embarrassed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32476</th>\n",
       "      <td>GLASGOW, Scotland (Reuters) - Most U.S. presid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  label\n",
       "22216  21st Century Wire says Ben Stein, reputable pr...      0\n",
       "27917  WASHINGTON (Reuters) - U.S. President Donald T...      1\n",
       "25007  (Reuters) - Puerto Rico Governor Ricardo Rosse...      1\n",
       "1377   On Monday, Donald Trump once again embarrassed...      0\n",
       "32476  GLASGOW, Scotland (Reuters) - Most U.S. presid...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset loader, sampling, and a small demo dataset\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    data_dir: Path = DATA_DIR, max_rows: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Load dataset by combining `fake.csv` (label 0) and `true.csv` (label 1).\"\"\"\n",
    "    fake_path = data_dir / \"fake.csv\"\n",
    "    true_path = data_dir / \"true.csv\"\n",
    "\n",
    "    if fake_path.exists() and true_path.exists():\n",
    "        try:\n",
    "            fake = pd.read_csv(fake_path)\n",
    "            true = pd.read_csv(true_path)\n",
    "\n",
    "            # Try to find an `article` column; fallback to first column\n",
    "            def get_text_col(df):\n",
    "                for c in [\"article\", \"text\", \"content\"]:\n",
    "                    if c in df.columns:\n",
    "                        return df[c].astype(str)\n",
    "                return df.iloc[:, 0].astype(str)\n",
    "\n",
    "            fake_text = get_text_col(fake)\n",
    "            true_text = get_text_col(true)\n",
    "            fake_df = pd.DataFrame({\"article\": fake_text, \"label\": 0})\n",
    "            true_df = pd.DataFrame({\"article\": true_text, \"label\": 1})\n",
    "            df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "            if max_rows is not None:\n",
    "                df = df.sample(n=min(len(df), max_rows), random_state=DEFAULT_SEED)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read CSVs: {e}\")\n",
    "\n",
    "    # Fallback synthetic dataset\n",
    "    logger.info(\"Using synthetic demo dataset (small).\")\n",
    "    demo = [\n",
    "        (\n",
    "            \"Government releases new public health guidelines that are consistent with prior research and expert consensus.\",\n",
    "            1,\n",
    "        ),\n",
    "        (\n",
    "            \"Aliens landed on the White House lawn last night, multiple sources confirm\",\n",
    "            0,\n",
    "        ),\n",
    "        (\n",
    "            \"Scientific team publishes peer-reviewed article showing new vaccine efficacy.\",\n",
    "            1,\n",
    "        ),\n",
    "        (\n",
    "            \"Miracle cure for diabetes discovered in backyard herb; no clinical trials yet\",\n",
    "            0,\n",
    "        ),\n",
    "    ]\n",
    "    df = pd.DataFrame(demo, columns=[\"article\", \"label\"])\n",
    "    if max_rows:\n",
    "        df = df.head(max_rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sample_k(\n",
    "    df: pd.DataFrame, k: int, mode: str = \"random\", seed: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    if seed is None:\n",
    "        seed = DEFAULT_SEED\n",
    "    if mode == \"random\":\n",
    "        return df.sample(n=min(k, len(df)), random_state=seed).reset_index(drop=True)\n",
    "    elif mode == \"first\":\n",
    "        return df.head(k).reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'random' or 'first'\")\n",
    "\n",
    "\n",
    "# Quick demo load\n",
    "dataset = load_dataset(max_rows=200)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b330b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 0\n",
      "1 -> 1\n",
      "The answer is 0. -> 0\n",
      "Trustworthy -> 1\n",
      "This seems fake. -> 0\n"
     ]
    }
   ],
   "source": [
    "# Batched synchronous generation and streaming demo\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def generate_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    device: str = DEVICE,\n",
    "    max_new_tokens: int = 32,\n",
    "    temperature: float = 0.0,\n",
    "    do_sample: bool = False,\n",
    "    batch_size: int = 4,\n",
    ") -> List[str]:\n",
    "    outputs = []\n",
    "    model_device = next(model.parameters()).device\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i : i + batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch_prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        input_ids = enc.input_ids.to(model_device)\n",
    "        attention_mask = enc.attention_mask.to(model_device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=do_sample,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        # For each generated sequence, decode only the generated portion\n",
    "        for ids, prompt in zip(generated_ids, batch_prompts):\n",
    "            text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            # Remove the prompt prefix if present\n",
    "            if prompt.strip() and text.startswith(prompt.strip()):\n",
    "                text = text[len(prompt.strip()) :].strip()\n",
    "            outputs.append(text)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def streaming_infer_simple(model, tokenizer, prompt: str, max_new_tokens: int = 20):\n",
    "    \"\"\"A simple (inefficient) streaming generator for demo: generates 1 token at a time by increasing max_new_tokens.\"\"\"\n",
    "    partial = \"\"\n",
    "    for step in range(1, max_new_tokens + 1):\n",
    "        out = generate_batch(model, tokenizer, [prompt], max_new_tokens=step)[0]\n",
    "        if out == partial:\n",
    "            # no new tokens\n",
    "            continue\n",
    "        partial = out\n",
    "        yield partial\n",
    "\n",
    "\n",
    "# Extraction of classification label\n",
    "\n",
    "\n",
    "def extract_classification(text: str) -> Optional[int]:\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    # Try to find an explicit 0/1 token\n",
    "    m = re.search(r\"\\b([01])\\b\", text)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    # Try to find words\n",
    "    txt = text.lower()\n",
    "    if \"untrust\" in txt or \"fake\" in txt or \"not\" in txt and \"trust\" in txt:\n",
    "        return 0\n",
    "    if \"trust\" in txt or \"true\" in txt or \"reliable\" in txt:\n",
    "        return 1\n",
    "    return None\n",
    "\n",
    "\n",
    "# Demo of extraction\n",
    "tests = [\"0\", \"1\", \"The answer is 0.\", \"Trustworthy\", \"This seems fake.\"]\n",
    "for t in tests:\n",
    "    print(t, \"->\", extract_classification(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f310047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.75, 'confusion_matrix': array([[2, 0, 0],\n",
      "       [0, 1, 1],\n",
      "       [0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Metrics and plotting\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: List[int], y_pred: List[Optional[int]]) -> Dict[str, Any]:\n",
    "    # Convert None to a special class or treat as incorrect\n",
    "    y_pred_clean = [(p if p is not None else -1) for p in y_pred]\n",
    "    # For accuracy, treat None as incorrect\n",
    "    valid_mask = [p in (0, 1) for p in y_pred]\n",
    "    acc = accuracy_score(y_true, [p if p in (0, 1) else -1 for p in y_pred_clean])\n",
    "    cm = confusion_matrix(\n",
    "        y_true, [p if p in (0, 1) else 2 for p in y_pred_clean], labels=[0, 1, 2]\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"confusion_matrix\": cm}\n",
    "\n",
    "\n",
    "def plot_metric_comparison(metrics_by_model: Dict[str, Dict[str, Any]]):\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {\"model\": m, \"accuracy\": metrics[\"accuracy\"]}\n",
    "            for m, metrics in metrics_by_model.items()\n",
    "        ]\n",
    "    )\n",
    "    sns.barplot(data=df, x=\"model\", y=\"accuracy\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Model accuracy comparison\")\n",
    "\n",
    "\n",
    "# Example usage with dummy preds\n",
    "print(compute_metrics([1, 0, 1, 0], [1, 0, None, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2994bd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_pipeline:Using cached model for gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5, 'confusion_matrix': array([[0, 2, 0],\n",
      "       [0, 4, 2],\n",
      "       [0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Full pipeline runner for a model name (small, non-optimized demo)\n",
    "\n",
    "\n",
    "def run_pipeline_on_dataset(\n",
    "    model_name: str,\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 20,\n",
    "    sample_mode: str = \"random\",\n",
    "    seed: Optional[int] = None,\n",
    "):\n",
    "    model, tokenizer = load_model_safe(model_name)\n",
    "    samples = sample_k(df, k=k, mode=sample_mode, seed=seed)\n",
    "    prompts = [make_prompt(a) for a in samples.article.tolist()]\n",
    "    outputs = generate_batch(model, tokenizer, prompts, batch_size=4)\n",
    "    preds = [extract_classification(o) for o in outputs]\n",
    "    metrics = compute_metrics(samples.label.tolist(), preds)\n",
    "    return {\"model\": model_name, \"metrics\": metrics, \"preds\": preds, \"samples\": samples}\n",
    "\n",
    "\n",
    "# Run on smoke model for a small k\n",
    "demo_res = run_pipeline_on_dataset(\"gpt2\", dataset, k=8)\n",
    "print(demo_res[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76df364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Unit tests (pytest style) - small subset\n",
    "\n",
    "# Note: Running pytest in-notebook is possible via `!pytest -q` but here we provide small asserts for demo.\n",
    "\n",
    "\n",
    "def _test_tokenizer_roundtrip():\n",
    "    tok = default_tokenizer\n",
    "    txt = \"Round trip test\"\n",
    "    enc = tok(txt)\n",
    "    dec = tok.decode(enc.input_ids)\n",
    "    assert isinstance(dec, str)\n",
    "\n",
    "\n",
    "def _test_extract_classification():\n",
    "    assert extract_classification(\"0\") == 0\n",
    "    assert extract_classification(\"1\") == 1\n",
    "    assert extract_classification(\"This is fake news\") == 0\n",
    "    assert extract_classification(\"Reliable and trustworthy\") == 1\n",
    "\n",
    "\n",
    "_test_tokenizer_roundtrip()\n",
    "_test_extract_classification()\n",
    "print(\"Basic tests passed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
